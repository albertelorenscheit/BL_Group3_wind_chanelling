{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask.dataframe as ddf\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from channeling_lib import AWS_file_loader, load_path\n",
    "\n",
    "stations_str = ['Tom Joad','Rosanna','Bette Davis', 'Layla', 'Mrs Robinson']\n",
    "\n",
    "# TinyTag_str = ['CEB_1', 'CEB_2', 'CEB_3', 'CEB_4', 'CEB_5', 'TH1', 'TH2', 'TH3', 'TH5', 'TH6', 'TH8', 'TT1', 'TT2', 'TT3', 'TT4', 'TT5', 'TT6', 'TT7', 'TT9', 'TT12', 'TT13', 'TT14', 'TT15', 'TT16', 'TT17', 'TT18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = load_path()+'RawData/TinyTag/'\n",
    "\n",
    "instrument_textbook_name = load_path()+'instrument_textbook_BLcourse_spring2025(BL instruments).csv'\n",
    "instrument_textbook_data = pd.read_csv(instrument_textbook_name, encoding='latin1')\n",
    "\n",
    "# instrument_textbook_data['Station name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract station names (keep only prefix + number)\n",
    "instrument_textbook_data['Formatted Name'] = [\n",
    "    re.sub(r\"(TT|TH|CEB)(\\d+).*\", r\"\\1\\2\", name) if re.match(r\"(TT|TH|CEB)\\d+\", name) else name\n",
    "    for name in instrument_textbook_data['Station name']\n",
    "]\n",
    "\n",
    "# Step 2: Find duplicate names\n",
    "duplicates = instrument_textbook_data['Formatted Name'].value_counts()\n",
    "duplicate_names = duplicates[duplicates > 1].index  # Names that appear more than once\n",
    "\n",
    "# Step 3: Apply \"_low\" and \"_high\" based on \"Th height (m)\"\n",
    "for name in duplicate_names:\n",
    "    subset = instrument_textbook_data[instrument_textbook_data['Formatted Name'] == name]\n",
    "    \n",
    "    # Find the lowest and highest Th height\n",
    "    min_index = subset['Th height (m)'].idxmin()\n",
    "    max_index = subset['Th height (m)'].idxmax()\n",
    "    \n",
    "    # Rename them\n",
    "    instrument_textbook_data.loc[min_index, 'Formatted Name'] = f\"{name}_low\"\n",
    "    instrument_textbook_data.loc[max_index, 'Formatted Name'] = f\"{name}_high\"\n",
    "\n",
    "# Step 4: Get the final formatted list\n",
    "formatted_names = instrument_textbook_data['Formatted Name'].tolist()\n",
    "\n",
    "# formatted_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert 'Formatted Name' as the first column\n",
    "instrument_textbook_data.insert(0, 'Formatted Station Name', instrument_textbook_data.pop('Formatted Name'))\n",
    "\n",
    "\n",
    "# # Check if 'Formatted Name' exists, if not, create it\n",
    "# if 'Formatted Name' not in instrument_textbook_data.columns:\n",
    "#     instrument_textbook_data['Formatted Name'] = instrument_textbook_data['Station name']  # Default to 'Station name' if missing\n",
    "\n",
    "# # Remove 'Formatted Station Name' if it already exists\n",
    "# if 'Formatted Station Name' in instrument_textbook_data.columns:\n",
    "#     instrument_textbook_data.drop(columns=['Formatted Station Name'], inplace=True)\n",
    "\n",
    "# # Insert 'Formatted Name' as the first column under 'Formatted Station Name'\n",
    "# instrument_textbook_data.insert(0, 'Formatted Station Name', instrument_textbook_data.pop('Formatted Name'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading start and end times of Tinytag for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TinyTag_str = instrument_textbook_data['Station name'].tolist()\n",
    "TinyTag_str = [s for s in formatted_names if s.startswith((\"TT\", \"TH\", \"CEB\"))]\n",
    "\n",
    "\n",
    "# TinyTag_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instrument_textbook_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-01-26 13:56:00')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(instrument_textbook_data.loc[instrument_textbook_data['Formatted Station Name'] == 'TT12_low', 'Setup time (UTC)'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_times = {}\n",
    "\n",
    "for station in TinyTag_str:\n",
    "    setup_time_idx = pd.to_datetime(instrument_textbook_data.loc[instrument_textbook_data['Formatted Station Name'] == station, 'Setup time (UTC)'].values[0])\n",
    "\n",
    "    maintenance_start_time_idx = pd.to_datetime(instrument_textbook_data.loc[instrument_textbook_data['Formatted Station Name'] == station, 'Maintenance start time (UTC)'].values[0])\n",
    "\n",
    "    # maintenance_duration_idx = int(instrument_textbook_data.loc[instrument_textbook_data['Formatted Station Name'] == station, 'Maintenance duration (minutes)'].values[0])\n",
    "    maintenance_duration_value = instrument_textbook_data.loc[instrument_textbook_data['Formatted Station Name'] == station, 'Maintenance duration (minutes)'].values[0]\n",
    "    maintenance_duration_idx = int(maintenance_duration_value) if not pd.isna(maintenance_duration_value) else 0\n",
    "\n",
    "    retrieval_time_idx = pd.to_datetime(instrument_textbook_data.loc[instrument_textbook_data['Formatted Station Name'] == station, 'Retrieval time (UTC)'].values[0])\n",
    "\n",
    "    manual_times[station] = {\n",
    "            'setup_time': setup_time_idx,\n",
    "            'maintenance_start_time': maintenance_start_time_idx,\n",
    "            'maintenance_duration': maintenance_duration_idx,\n",
    "            'retrieval_time': retrieval_time_idx\n",
    "        }\n",
    "\n",
    "# manual_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data based on the start end and duration times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_based_on_time(df, setup_time, maintenance_start_time, maintenance_duration, retrieval_time, is_second_file=False):\n",
    "    \"\"\"\n",
    "    Filtert die Daten basierend auf den angegebenen Zeitbereichen f端r das 1. und 2. Dataset.\n",
    "    \n",
    "    :param df: Der DataFrame mit den Rohdaten.\n",
    "    :param setup_time: Der Setup-Zeitpunkt in UTC (als pd.Timestamp).\n",
    "    :param maintenance_start_time: Der Startzeitpunkt der Wartung in UTC (als pd.Timestamp).\n",
    "    :param maintenance_duration: Die Dauer der Wartung in Minuten.\n",
    "    :param retrieval_time: Der Retrieval-Zeitpunkt in UTC (als pd.Timestamp).\n",
    "    :param is_second_file: True, wenn es sich um das 2. Dataset handelt, andernfalls False f端r das 1. Dataset.\n",
    "    :return: Der gefilterte DataFrame.\n",
    "    \"\"\"\n",
    "    if is_second_file:\n",
    "        # Filter f端r das 2. Dataset: nach Maintenance start time + duration + 5 Minuten und Retrieval time - 5 Minuten\n",
    "        start_time = maintenance_start_time + pd.Timedelta(minutes=int(maintenance_duration))\n",
    "        end_time = retrieval_time\n",
    "    else:\n",
    "        # Filter f端r das 1. Dataset: nach Setup time + 5 Minuten und Maintenance start time - 5 Minuten\n",
    "        start_time = setup_time\n",
    "        end_time = maintenance_start_time\n",
    "\n",
    "    # Filter die Daten innerhalb des angegebenen Zeitrahmens\n",
    "    filtered_df = df[(df.index >= start_time) & (df.index <= end_time)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading TinyTag Calibration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration data is stored in format:\n",
    "\n",
    "- CEB_i_calibration_data\n",
    "\n",
    "- THi_calibration_data\n",
    "\n",
    "- TTi_calibration_data\n",
    "\n",
    "Where i corresponds to the number of the TinyTag, i.e. the same number as the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function from unis github\n",
    "def read_Tinytag(filename, sensor):\n",
    "    '''\n",
    "    Reads data from one or several data files from the Tinytag output files.\n",
    "\n",
    "    Parameters:\n",
    "    -------\n",
    "    filename: str\n",
    "        String with path to file(s)\n",
    "        If several files shall be read, specify a string including UNIX-style wildcards\n",
    "    sensor: str\n",
    "        One of \"TT\", \"TH\" or \"CEB\"\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        a pandas dataframe with time as index and the individual variables as columns.\n",
    "    '''\n",
    "\n",
    "    import dask.dataframe as ddf\n",
    "\n",
    "\n",
    "    if sensor == \"TT\":\n",
    "        df = ddf.read_csv(filename, delimiter=\"\\t\", skiprows=5, parse_dates=[1], date_format=\"%d %b %Y %H:%M:%S\", names=[\"RECORD\", \"TIMESTAMP\", \"T_black\", \"T_white\"], encoding = \"ISO-8859-1\")\n",
    "    elif sensor == \"TH\":\n",
    "        df = ddf.read_csv(filename, delimiter=\"\\t\", skiprows=5, parse_dates=[1], date_format=\"%d %b %Y %H:%M:%S\", names=[\"RECORD\", \"TIMESTAMP\", \"T\", \"RH\"], encoding = \"ISO-8859-1\")\n",
    "    elif sensor == \"CEB\":\n",
    "        df = ddf.read_csv(filename, delimiter=\"\\t\", skiprows=5, parse_dates=[1], date_format=\"%d %b %Y %H:%M:%S\", names=[\"RECORD\", \"TIMESTAMP\", \"T\"], encoding = \"ISO-8859-1\")\n",
    "    else:\n",
    "        assert False, 'Sensortype of Tinytag not known. Should be one of \"TT\", \"TH\" or \"CEB\".'\n",
    "\n",
    "    df = df.compute()\n",
    "    df.set_index(\"TIMESTAMP\", inplace=True)\n",
    "\n",
    "    for key in list(df.columns):\n",
    "        if key == \"RECORD\":\n",
    "            pass\n",
    "        else:\n",
    "            data = [float(i.split(\" \")[0]) for i in df[key]]\n",
    "            unit = df[key].iloc[0].split(\" \")[1]\n",
    "            if unit == \"属C\":\n",
    "                unit = \"degC\"\n",
    "            new_key = f\"{key}_{unit}\"\n",
    "\n",
    "            df[new_key] = data\n",
    "\n",
    "            df.drop(key, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CEB_i folders inside TinyTag\n",
    "folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f)) and f.startswith('CEB_')]\n",
    "\n",
    "# Dictionary to store the datasets\n",
    "calibration_data = {}\n",
    "\n",
    "# Loop through each CEB_i folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    \n",
    "    # Get all .txt files that start with \"CEB_i_calibration_\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(folder + \"_calibration_\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Run the read_Tinytag function\n",
    "        dataset = read_Tinytag(file_path, 'CEB')\n",
    "        \n",
    "        # Store the dataset in a dictionary using the folder name as the key\n",
    "        calibration_data[f\"{folder}_calibration_data\"] = dataset\n",
    "\n",
    "# Loop over the stored datasets and assign them as individual variables\n",
    "for dataset_name in calibration_data.keys():\n",
    "    globals()[dataset_name] = calibration_data[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TH\n",
    "folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f)) and f.startswith('TH')]\n",
    "\n",
    "# Dictionary to store the datasets\n",
    "calibration_data = {}\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    \n",
    "    # Get all .txt files that start with \"THi_calibration_\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(folder + \"_calibration_\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Run the read_Tinytag function\n",
    "        dataset = read_Tinytag(file_path, 'TH')\n",
    "        \n",
    "        # Store the dataset in a dictionary using the folder name as the key\n",
    "        calibration_data[f\"{folder}_calibration_data\"] = dataset\n",
    "\n",
    "for dataset_name in calibration_data.keys():\n",
    "    globals()[dataset_name] = calibration_data[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TT\n",
    "folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f)) and f.startswith('TT')]\n",
    "\n",
    "# Dictionary to store the datasets\n",
    "calibration_data = {}\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    \n",
    "    # Get all .txt files that start with \"TTi_calibration_\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(folder + \"_calibration_\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Run the read_Tinytag function\n",
    "        dataset = read_Tinytag(file_path, 'TT')\n",
    "        \n",
    "        # Store the dataset in a dictionary using the folder name as the key\n",
    "        calibration_data[f\"{folder}_calibration_data\"] = dataset\n",
    "\n",
    "for dataset_name in calibration_data.keys():\n",
    "    globals()[dataset_name] = calibration_data[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading TinyTag data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CEB_i folders inside TinyTag\n",
    "folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f)) and f.startswith('CEB_')]\n",
    "\n",
    "# Dictionary to store the datasets\n",
    "calibration_data = {}\n",
    "\n",
    "# Loop through each CEB_i folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    \n",
    "    # Get all .txt files that does not include \"CEB_i_calibration_\"\n",
    "    # files = [f for f in os.listdir(folder_path) if f.startswith(folder + \"_calibration_\") and f.endswith(\".txt\")]\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\") and \"_calibration_\" not in f]\n",
    "\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Run the read_Tinytag function\n",
    "        dataset = read_Tinytag(file_path, 'CEB')\n",
    "        \n",
    "        # Store the dataset in a dictionary using the folder name as the key\n",
    "        calibration_data[f\"{folder}_calibration_data\"] = dataset\n",
    "\n",
    "# Loop over the stored datasets and assign them as individual variables\n",
    "for dataset_name in calibration_data.keys():\n",
    "    globals()[dataset_name] = calibration_data[dataset_name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boundary_layer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
